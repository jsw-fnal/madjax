{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax import serialization\n",
    "import optax\n",
    "\n",
    "from SmoothNF import SmoothNormalizingFlow \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "from functools import partial\n",
    "from typing import Sequence\n",
    "\n",
    "import timeit\n",
    "import pickle\n",
    "\n",
    "\n",
    "from jax.config import config; #config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# physics stuff\n",
    "import pylhe\n",
    "\n",
    "# Madjax\n",
    "import madjax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 300\n",
    "num_warm_up_epochs = 10\n",
    "steps_per_epoch = 75 #20000 // batch_size\n",
    "\n",
    "learning_rate = 0.5e-4\n",
    "\n",
    "eps_border = 1.0e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Setup MadJax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "E_cm = 500.\n",
    "config_name  = \"ee_ttbar_bqq_bqq\"\n",
    "process_name = \"Matrix_1_epem_ttx_t_budx_tx_bxdux\"\n",
    "nDimPS=14\n",
    "\n",
    "mj = madjax.MadJax(config_name=config_name)\n",
    "matrix_element = mj.matrix_element(E_cm=E_cm, process_name=process_name, return_grad=False, do_jit=False)\n",
    "jacobian = mj.jacobian(E_cm=E_cm, process_name=process_name, do_jit=False)\n",
    "me_and_jac = mj.matrix_element_and_jacobian(E_cm=E_cm, process_name=process_name)\n",
    "ps_gen = mj.phasespace_generator(E_cm=E_cm, process_name=process_name)({})\n",
    "ps_vec = mj.phasespace_vectors(E_cm=E_cm, process_name=process_name)\n",
    "\n",
    "external_params={}\n",
    "\n",
    "sigma_smear=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_rv_from_ps_point(ps_point, E_cm):\n",
    "    i_rv, i_wt = ps_gen.invertKinematics(E_cm, [madjax.phasespace.vectors.Vector(p) for p in ps_point])\n",
    "    \n",
    "    return jnp.array(i_rv)\n",
    "\n",
    "vector_rv_from_ps_point = jax.jit(jax.vmap(scalar_rv_from_ps_point, in_axes=(0,None)), static_argnums=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scalar_log_me(params):\n",
    "    _eps = 1.0e-10\n",
    "    def func(rv):\n",
    "        me_val, jac_val = me_and_jac(params, rv)\n",
    "        return jnp.log(me_val)+ jnp.log(jac_val)\n",
    "        #return jnp.log(matrix_element(params, rv)+_eps)+jnp.log(jacobian(params, rv)+_eps)\n",
    "    return func\n",
    "\n",
    "scalar_log_me = get_scalar_log_me(params={})\n",
    "vector_log_me = jax.vmap(get_scalar_log_me(params={}))\n",
    "\n",
    "grad_scalar_log_me = jax.grad(get_scalar_log_me(params={}))\n",
    "grad_vector_log_me = jax.vmap(jax.grad(get_scalar_log_me(params={})))\n",
    "\n",
    "\n",
    "scalar_log_me_jit = jax.jit(get_scalar_log_me(params={}))\n",
    "vector_log_me_jit = jax.jit(jax.vmap(get_scalar_log_me(params={})))\n",
    "\n",
    "grad_scalar_log_me_jit = jax.jit(jax.grad(get_scalar_log_me(params={})))\n",
    "grad_vector_log_me_jit = jax.jit(jax.vmap(jax.grad(get_scalar_log_me(params={}))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prejit_matrix_elements():\n",
    "    init_data = 0.9*jnp.ones((10,nDimPS))\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    scalar_log_me_jit(init_data[0])\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"scalar_log_me time\", elapsed,\"\\n\")\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    vector_log_me_jit(init_data)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"vector_log_me time\", elapsed,\"\\n\")\n",
    "    \n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "    grad_scalar_log_me_jit(init_data[0])\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"grad_scalar_log_me time\", elapsed,\"\\n\")\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    grad_vector_log_me_jit(init_data)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"grad_vector_log_me time\", elapsed,\"\\n\")\n",
    "    \n",
    "if True:\n",
    "    prejit_matrix_elements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LHE Event Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get LHE events generator\n",
    "lhe_events = pylhe.readLHE(\"./data/ee_ttbar_bqq_bqq/unweighted_events.lhe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lhe_event_to_ps_point(event):\n",
    "    _particles = []\n",
    "    for p in event.particles:\n",
    "        #print(p.id, p.status, p.px, p.py, p.pz, p.e)\n",
    "        if p.status==-1 or p.status==1:\n",
    "            _particles += [[p.e, p.px, p.py, p.pz]]\n",
    "    return jnp.array(_particles)\n",
    "\n",
    "def get_multiple_lhe_event_to_ps_point(lhe_event_generator, n_events):\n",
    "    _evts=[]\n",
    "    for i in range(n_events):\n",
    "        try:\n",
    "            _ev = lhe_event_generator.__next__()\n",
    "        except:\n",
    "            print(\"No More Events\")\n",
    "            return _evts\n",
    "        \n",
    "        _ps = lhe_event_to_ps_point(_ev)\n",
    "        _evts += [_ps]\n",
    "    \n",
    "    return np.array(_evts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = get_multiple_lhe_event_to_ps_point(lhe_events, 10000)\n",
    "dataset = vector_rv_from_ps_point(events, E_cm)\n",
    "\n",
    "train_data = dataset[0:8000]\n",
    "test_data = dataset[8000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Smooth NF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1():\n",
    "    return SmoothNormalizingFlow(num_flows = 4,\n",
    "                                num_biject = 4,\n",
    "                                num_in_feat = nDimPS,\n",
    "                                cond_mlp_width = [[100,100]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#def force_MSE(x, grad_flow_logprob):\n",
    "#    grad_logME = grad_vector_log_me(x)\n",
    "#    return jnp.square(grad_logME - grad_flow_logprob).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step1(state, batch, key):\n",
    "    def loss_fn(params):\n",
    "        def _logp(x):\n",
    "            z, ldj = model1().apply({'params':params}, x, method=model1().inverse_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp + ldj)\n",
    "\n",
    "        def synth_samp_logp_minus_logME(z):\n",
    "            x, ldj = model1().apply({'params':params}, z, method=model1().forward_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp - ldj) - vector_log_me(x).reshape(-1,1)\n",
    "                \n",
    "        forward_kld_loss = (-1.0*_logp(batch)).mean()\n",
    "        \n",
    "        synth_z = jax.random.uniform(key, shape=batch.shape, minval=1.0e-10, maxval=(1.0-1.0e-10))\n",
    "        reverse_kld_loss = (synth_samp_logp_minus_logME(synth_z)).mean()\n",
    "                \n",
    "       \n",
    "        loss = forward_kld_loss + 0.1*reverse_kld_loss # + 0.001*force_MSE_loss\n",
    "        return loss\n",
    "    \n",
    "   \n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    \n",
    "    return state.apply_gradients(grads=grads)\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "@jax.jit\n",
    "def train_step1_only_fkl(state, batch, key):\n",
    "    def loss_fn(params):\n",
    "        def _logp(x):\n",
    "            z, ldj = model1().apply({'params':params}, x, method=model1().inverse_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp + ldj)\n",
    "                \n",
    "        forward_kld_loss = (-1.0*_logp(batch)).mean()\n",
    "       \n",
    "        loss = forward_kld_loss \n",
    "        return loss\n",
    "    \n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    \n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval1(params, batch, key):\n",
    "    def eval_model(smoothnf):\n",
    "        def _logp(x):\n",
    "            z, ldj = model1().apply({'params':params}, x, method=model1().inverse_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp + ldj)\n",
    "\n",
    "        def synth_samp_logp_minus_logME(z):\n",
    "            x, ldj = model1().apply({'params':params}, z, method=model1().forward_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp - ldj) - vector_log_me(x).reshape(-1,1)\n",
    "        \n",
    "        def _force_MSE_and_cos(x, grad_flow_logprob):\n",
    "            grad_logME = grad_vector_log_me_jit(x)\n",
    "            _force = jnp.square(grad_logME - grad_flow_logprob).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1)\n",
    "            \n",
    "            gme_norm = jnp.sqrt(jnp.square(grad_logME).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1))\n",
    "            gfl_norm = jnp.sqrt(jnp.square(grad_flow_logprob).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1))\n",
    "            _cos = jnp.multiply(grad_logME, grad_flow_logprob) / (gme_norm + 1e-10) / (gfl_norm + 1e-10)\n",
    "            \n",
    "            return _force, _cos\n",
    "            \n",
    "        \n",
    "                \n",
    "        forward_kld_loss = (-1.0*_logp(batch)).mean()\n",
    "        \n",
    "        synth_z = jax.random.uniform(key, shape=batch.shape, minval=1e-10, maxval=(1.0-1e-10))\n",
    "        reverse_kld_loss = synth_samp_logp_minus_logME(synth_z).mean()\n",
    "        \n",
    "        _, _gradx_logp = model1().apply({'params':params}, batch, method=model1().val_and_gradx_logprob)\n",
    "        _f, _c = _force_MSE_and_cos(batch, _gradx_logp)\n",
    "        force_mse = _f.mean()\n",
    "        cos_loss = _c.mean()\n",
    "                \n",
    "        metrics = {'fkld': forward_kld_loss,\n",
    "                   'rkld': reverse_kld_loss,\n",
    "                   'force': force_mse,\n",
    "                   'cos': cos_loss,\n",
    "                  }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    return nn.apply(eval_model, model1())({'params': params})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhe_events = pylhe.readLHE(\"./data/ee_ttbar_bqq_bqq/unweighted_events.lhe\")\n",
    "# events = get_multiple_lhe_event_to_ps_point(lhe_events, 10000)\n",
    "# dataset1 = vector_rv_from_ps_point(events, E_cm)\n",
    "# train_data1 = dataset1[0:8000]\n",
    "# test_data1 = dataset1[8000:]\n",
    "\n",
    "train_data1 = train_data\n",
    "test_data1 = test_data\n",
    "\n",
    "rng, key, eval_rng = jax.random.split(key, 3)\n",
    "\n",
    "#batch_size = 50\n",
    "#learning_rate = 1e-3\n",
    "init_data = jnp.ones((batch_size,nDimPS))\n",
    "\n",
    "\n",
    "#optimizer1 = optax.chain( optax.zero_nans(), optax.adam(learning_rate), optax.zero_nans())\n",
    "\n",
    "state1_init = train_state.TrainState.create(\n",
    "      apply_fn=model1().apply,\n",
    "      params=model1().init(key, init_data)['params'],\n",
    "      tx=optax.adam(learning_rate),\n",
    "  )\n",
    "\n",
    "state1 = state1_init\n",
    "\n",
    "#prejit:\n",
    "print(\"pre-jit model1\")\n",
    "batch = train_data1[np.random.choice(np.arange(len(train_data1)), size = batch_size)]\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "train_step1(state1, batch, key)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"train_step1: elapsed time\", elapsed,\"\\n\")\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "train_step1_only_fkl(state1, batch, key)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"train_step1_norkl: elapsed time\", elapsed,\"\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pre-jit eval1\")\n",
    "start_time = timeit.default_timer()\n",
    "metrics = eval1(state1.params, test_data1, eval_rng)\n",
    "print('eval1, fwdKLD: {:.4f}, revKLD: {:.4f}, forceMSE: {:.4f}, CosLoss: {:.4f} \\n'.format(\n",
    "        metrics['fkld'], metrics['rkld'], metrics['force'], metrics['cos']))\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"elapsed time\", elapsed,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_params1 = []\n",
    "saved_params1.append(state1.params)\n",
    "\n",
    "losses1 = []\n",
    "\n",
    "state1 = state1_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "# num_epochs = 100\n",
    "# steps_per_epoch = 75 #20000 // batch_size\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch\",epoch)\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        \n",
    "        batch = train_data1[np.random.choice(np.arange(len(train_data1)), size = batch_size)]\n",
    "        rng, key = jax.random.split(rng)\n",
    "        if epoch < num_warm_up_epochs:\n",
    "            state1 = train_step1_only_fkl(state1, batch, key)\n",
    "        else:\n",
    "            try:\n",
    "                state_update = train_step1(state1, batch, key)\n",
    "                state1 = state_update\n",
    "            except FloatingPointError:\n",
    "                print(\"### GRAD FloatingPointError - continue without update\")\n",
    "        \n",
    "    saved_params1.append(state1.params)\n",
    "        \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"elapsed time\", elapsed)\n",
    "    \n",
    "    metrics = eval1(state1.params, test_data1, eval_rng)\n",
    "    print('eval1 epoch: {}, fwdKLD: {:.4f}, revKLD: {:.4f}, forceMSE: {:.4f}, CosLoss: {:.4f} \\n'.format(\n",
    "        epoch, metrics['fkld'], metrics['rkld'], metrics['force'], metrics['cos']))\n",
    "    \n",
    "    losses1.append([ metrics['fkld'], metrics['rkld'], metrics['force'], metrics['cos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outfile_name = \"results_comp_smoothNF/model1_wk0.1_wf0_results2.pkl\"\n",
    "outfile = open(outfile_name,'wb')\n",
    "saved_param_dict = []\n",
    "for ip in saved_params1:\n",
    "    ip_dict = serialization.to_state_dict(ip)\n",
    "    saved_param_dict.append(ip_dict)\n",
    "\n",
    "outtuple = (saved_param_dict, losses1)\n",
    "pickle.dump(outtuple, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No RKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2():\n",
    "    return SmoothNormalizingFlow(num_flows = 4,\n",
    "                                num_biject = 4,\n",
    "                                num_in_feat = nDimPS,\n",
    "                                cond_mlp_width = [[100,100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step2(state, batch, key):\n",
    "    def loss_fn(params):\n",
    "        def _logp(x):\n",
    "            z, ldj = model2().apply({'params':params}, x, method=model2().inverse_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp + ldj)\n",
    "                \n",
    "        forward_kld_loss = (-1.0*_logp(batch)).mean()\n",
    "        \n",
    "       \n",
    "        loss = forward_kld_loss \n",
    "        return loss\n",
    "    \n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    \n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval2(params, batch, key):\n",
    "    def eval_model(smoothnf):\n",
    "        def _logp(x):\n",
    "            z, ldj = model2().apply({'params':params}, x, method=model2().inverse_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp + ldj)\n",
    "\n",
    "        def synth_samp_logp_minus_logME(z):\n",
    "            x, ldj = model2().apply({'params':params}, z, method=model2().forward_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp - ldj) - vector_log_me(x).reshape(-1,1)\n",
    "        \n",
    "        def _force_MSE_and_cos(x, grad_flow_logprob):\n",
    "            grad_logME = grad_vector_log_me_jit(x)\n",
    "            _force = jnp.square(grad_logME - grad_flow_logprob).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1)\n",
    "            \n",
    "            gme_norm = jnp.sqrt(jnp.square(grad_logME).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1))\n",
    "            gfl_norm = jnp.sqrt(jnp.square(grad_flow_logprob).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1))\n",
    "            _cos = jnp.multiply(grad_logME, grad_flow_logprob) / (gme_norm + 1e-10) / (gfl_norm + 1e-10)\n",
    "            \n",
    "            return _force, _cos\n",
    "\n",
    "                \n",
    "        forward_kld_loss = (-1.0*_logp(batch)).mean()\n",
    "        #forward_kld_loss = (-1.0*_logp).mean()\n",
    "        \n",
    "        synth_z = jax.random.uniform(key, batch.shape)\n",
    "        reverse_kld_loss = synth_samp_logp_minus_logME(synth_z).mean()\n",
    "        \n",
    "        _, _gradx_logp = model2().apply({'params':params}, batch, method=model2().val_and_gradx_logprob)\n",
    "        _f, _c = _force_MSE_and_cos(batch, _gradx_logp)\n",
    "        force_mse = _f.mean()\n",
    "        cos_loss = _c.mean()\n",
    "        \n",
    "        \n",
    "        metrics = {'fkld': forward_kld_loss,\n",
    "                   'rkld': reverse_kld_loss,\n",
    "                   'force': force_mse,\n",
    "                   'cos': cos_loss,\n",
    "                   'loss': forward_kld_loss + 0.1*reverse_kld_loss #+ 0.001*force_loss\n",
    "                  }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    return nn.apply(eval_model, model2())({'params': params})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhe_events = pylhe.readLHE(\"./data/ee_ttbar_bqq_bqq/unweighted_events.lhe\")\n",
    "# events = get_multiple_lhe_event_to_ps_point(lhe_events, 10000)\n",
    "# dataset2 = vector_rv_from_ps_point(events, E_cm)\n",
    "# train_data2 = dataset2[0:8000]\n",
    "# test_data2 = dataset2[8000:]\n",
    "\n",
    "train_data2 = train_data\n",
    "test_data2 = test_data\n",
    "\n",
    "rng, key, eval_rng = jax.random.split(key, 3)\n",
    "\n",
    "# batch_size = 50\n",
    "#learning_rate = 1e-3\n",
    "init_data = jnp.ones((batch_size,nDimPS))\n",
    "\n",
    "\n",
    "#optimizer2 = optax.chain( optax.zero_nans(), optax.adam(learning_rate), optax.zero_nans())\n",
    "\n",
    "state2_init = train_state.TrainState.create(\n",
    "      apply_fn=model2().apply,\n",
    "      params=model2().init(key, init_data)['params'],\n",
    "      tx=optax.adam(learning_rate),\n",
    "  )\n",
    "\n",
    "state2 = state2_init\n",
    "\n",
    "#prejit:\n",
    "print(\"pre-jit model2\")\n",
    "start_time = timeit.default_timer()\n",
    "batch = train_data2[np.random.choice(np.arange(len(train_data2)), size = batch_size)]\n",
    "train_step2(state2, batch, key)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"elapsed time\", elapsed,\"\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pre-jit eval2\")\n",
    "start_time = timeit.default_timer()\n",
    "metrics = eval2(state2.params, test_data2, eval_rng)\n",
    "print('eval2,  fwdKLD: {:.4f}, revKLD: {:.4f}, forceMSE: {:.4f}, cosLoss: {:.4f} \\n'.format(\n",
    "         metrics['fkld'], metrics['rkld'], metrics['force'], metrics['cos']))\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"elapsed time\", elapsed,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_params2 = []\n",
    "saved_params2.append(state2.params)\n",
    "\n",
    "losses2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "# num_epochs = 100\n",
    "# steps_per_epoch = 75 #20000 // batch_size\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch\",epoch)\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        \n",
    "        batch = train_data2[np.random.choice(np.arange(len(train_data2)), size = batch_size)]\n",
    "        rng, key = jax.random.split(rng)\n",
    "        state2 = train_step2(state2, batch, key)\n",
    "        \n",
    "    saved_params2.append(state2.params)\n",
    "        \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"elapsed time\", elapsed)\n",
    "    \n",
    "    metrics = eval2(state2.params, test_data2, eval_rng)\n",
    "    print('eval2 epoch: {}, fwdKLD: {:.4f}, revKLD: {:.4f}, forceMSE: {:.4f}, cosLoss: {:.4f} \\n'.format(\n",
    "        epoch,  metrics['fkld'], metrics['rkld'], metrics['force'], metrics['cos']))\n",
    "    \n",
    "    losses2.append([ metrics['fkld'], metrics['rkld'], metrics['force'], metrics['cos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_name = \"results_comp_smoothNF/model2_wk0_wf0_results2.pkl\"\n",
    "outfile = open(outfile_name,'wb')\n",
    "saved_param_dict = []\n",
    "for ip in saved_params2:\n",
    "    ip_dict = serialization.to_state_dict(ip)\n",
    "    saved_param_dict.append(ip_dict)\n",
    "\n",
    "outtuple = (saved_param_dict, losses2)\n",
    "pickle.dump(outtuple, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_epoch = jnp.array([range(num_epochs)]).squeeze()\n",
    "\n",
    "# m1_fkl = jnp.array(losses1)[:,1]\n",
    "# m1_rkl = jnp.array(losses1)[:,2]\n",
    "\n",
    "# m2_fkl = jnp.array(losses2)[:,1]\n",
    "# m2_rkl = jnp.array(losses2)[:,2]\n",
    "\n",
    "# plt.plot(x_epoch, m1_fkl, '-', c='#1f77b4', label=r'$\\omega_k=0.1$: Likelihood')\n",
    "# plt.plot(x_epoch, m1_rkl, '--', c='#1f77b4', alpha=0.5, label=r'$\\omega_k=0.1$: Rev DKL')\n",
    "# plt.plot(x_epoch, m2_fkl, '-', c='#ff7f0e', label=r'$\\omega_k=0$: Likelihood')\n",
    "# plt.plot(x_epoch, m2_rkl, '--', c='#ff7f0e', alpha=0.5, label=r'$\\omega_k=0$: Rev DKL')\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3():\n",
    "    return SmoothNormalizingFlow(num_flows = 4,\n",
    "                                num_biject = 4,\n",
    "                                num_in_feat = nDimPS,\n",
    "                                cond_mlp_width = [[100,100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def force_MSE(x, grad_flow_logprob):\n",
    "#     grad_logME = grad_vector_log_me_jit(x)\n",
    "#     return jnp.square(grad_logME - grad_flow_logprob).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step3(state, batch, key):\n",
    "    def loss_fn(params):\n",
    "        \n",
    "        def _logp(x):\n",
    "            z, ldj = model3().apply({'params':params}, x, method=model3().inverse_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp + ldj)\n",
    "        \n",
    "        def _force_MSE(x, grad_flow_logprob):\n",
    "            grad_logME = grad_vector_log_me_jit(x)\n",
    "            return jnp.square(grad_logME - grad_flow_logprob).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1)\n",
    "                \n",
    "        forward_kld_loss = (-1.0*_logp(batch)).mean()\n",
    "        \n",
    "        _, _gradx_logp = model3().apply({'params':params}, batch, method=model3().val_and_gradx_logprob)        \n",
    "        force_mse = _force_MSE(batch, _gradx_logp).mean()\n",
    "        \n",
    "       \n",
    "        loss = forward_kld_loss + (1.0e-6)*force_mse\n",
    "        return loss\n",
    "    \n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    \n",
    "    return state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step3_only_fkl(state, batch, key):\n",
    "    def loss_fn(params):\n",
    "        def _logp(x):\n",
    "            z, ldj = model3().apply({'params':params}, x, method=model3().inverse_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp + ldj)\n",
    "                \n",
    "        forward_kld_loss = (-1.0*_logp(batch)).mean()\n",
    "       \n",
    "        loss = forward_kld_loss \n",
    "        return loss\n",
    "    \n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    \n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval3(params, batch, key):\n",
    "    def eval_model(smoothnf):\n",
    "        \n",
    "        def _logp(x):\n",
    "            z, ldj = model3().apply({'params':params}, x, method=model3().inverse_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp + ldj)\n",
    "        \n",
    "        def synth_samp_logp_minus_logME(z):\n",
    "            x, ldj = model3().apply({'params':params}, z, method=model3().forward_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp - ldj) - vector_log_me(x).reshape(-1,1)\n",
    "        \n",
    "        def _force_MSE_and_cos(x, grad_flow_logprob):\n",
    "            grad_logME = grad_vector_log_me_jit(x)\n",
    "            _force = jnp.square(grad_logME - grad_flow_logprob).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1)\n",
    "            \n",
    "            gme_norm = jnp.sqrt(jnp.square(grad_logME).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1))\n",
    "            gfl_norm = jnp.sqrt(jnp.square(grad_flow_logprob).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1))\n",
    "            _cos = jnp.multiply(grad_logME, grad_flow_logprob) / (gme_norm + 1e-10) / (gfl_norm + 1e-10)\n",
    "            \n",
    "            return _force, _cos\n",
    "          \n",
    "        forward_kld_loss = (-1.0*_logp(batch)).mean()\n",
    "        #forward_kld_loss = (-1.0*_logp).mean()\n",
    "        \n",
    "        synth_z = jax.random.uniform(key, batch.shape)\n",
    "        reverse_kld_loss = synth_samp_logp_minus_logME(synth_z).mean()\n",
    "        \n",
    "        _, _gradx_logp = model3().apply({'params':params}, batch, method=model3().val_and_gradx_logprob)\n",
    "        _f, _c = _force_MSE_and_cos(batch, _gradx_logp)\n",
    "        force_mse = _f.mean()\n",
    "        cos_loss = _c.mean()\n",
    "        \n",
    "        metrics = {'fkld': forward_kld_loss,\n",
    "                   'rkld': reverse_kld_loss,\n",
    "                   'force': force_mse,\n",
    "                   'cos': cos_loss,\n",
    "                   'loss': forward_kld_loss + 0.1*reverse_kld_loss #+ 0.001*force_loss\n",
    "                  }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    return nn.apply(eval_model, model3())({'params': params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhe_events = pylhe.readLHE(\"./data/ee_ttbar_bqq_bqq/unweighted_events.lhe\")\n",
    "# events = get_multiple_lhe_event_to_ps_point(lhe_events, 10000)\n",
    "# dataset3 = vector_rv_from_ps_point(events, E_cm)\n",
    "# train_data3 = dataset3[0:8000]\n",
    "# test_data3 = dataset3[8000:]\n",
    "\n",
    "train_data3 = train_data\n",
    "test_data3 = test_data\n",
    "\n",
    "rng, key, eval_rng = jax.random.split(key, 3)\n",
    "\n",
    "# batch_size = 50\n",
    "#learning_rate = 1e-3\n",
    "init_data = jnp.ones((batch_size,nDimPS))\n",
    "\n",
    "#optimizer2 = optax.chain( optax.zero_nans(), optax.adam(learning_rate), optax.zero_nans())\n",
    "\n",
    "state3_init = train_state.TrainState.create(\n",
    "      apply_fn=model3().apply,\n",
    "      params=model3().init(key, init_data)['params'],\n",
    "      tx=optax.adam(learning_rate),\n",
    "  )\n",
    "\n",
    "state3 = state3_init\n",
    "\n",
    "\n",
    "#prejit:\n",
    "print(\"pre-eval model3\")\n",
    "batch = train_data3[np.random.choice(np.arange(len(train_data3)), size = batch_size)]\n",
    "start_time = timeit.default_timer()\n",
    "train_step3(state3, batch, key)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"train_step3: elapsed time\", elapsed,\"\\n\")\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "train_step3_only_fkl(state3, batch, key)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"train_step3_nograd:  elapsed time\", elapsed,\"\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pre-jit eval3\")\n",
    "start_time = timeit.default_timer()\n",
    "metrics = eval3(state3.params, test_data3, eval_rng)\n",
    "print('eval3,  fwdKLD: {:.4f}, revKLD: {:.4f}, forceMSE: {:.4f}, cosLoss: {:.4f} \\n'.format(\n",
    "         metrics['fkld'], metrics['rkld'], metrics['force'], metrics['cos']))\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"elapsed time\", elapsed,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_params3 = []\n",
    "saved_params3.append(state3_init.params)\n",
    "\n",
    "losses3 = []\n",
    "\n",
    "state3 = state3_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 100\n",
    "# steps_per_epoch = 75 #20000 // batch_size\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch\",epoch)\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        \n",
    "        batch = train_data3[np.random.choice(np.arange(len(train_data3)), size = batch_size)]\n",
    "        rng, key = jax.random.split(rng)\n",
    "        if epoch < num_warm_up_epochs:\n",
    "            state3 = train_step3_only_fkl(state3, batch, key)\n",
    "        else:\n",
    "            state3 = train_step3(state3, batch, key)\n",
    "        \n",
    "    saved_params3.append(state3.params)\n",
    "        \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"elapsed time\", elapsed)\n",
    "    \n",
    "    metrics = eval3(state3.params, test_data3, eval_rng)\n",
    "    print('eval3 epoch: {}, fwdKLD: {:.4f}, revKLD: {:.4f}, forceMSE: {:.4f}, cosLoss: {:.4f} \\n'.format(\n",
    "        epoch,  metrics['fkld'], metrics['rkld'], metrics['force'], metrics['cos']))\n",
    "    \n",
    "    losses3.append([metrics['fkld'], metrics['rkld'], metrics['force'], metrics['cos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_name = \"results_comp_smoothNF/model3_wk0_wf1Em6_results2.pkl\"\n",
    "outfile = open(outfile_name,'wb')\n",
    "saved_param_dict = []\n",
    "for ip in saved_params3:\n",
    "    ip_dict = serialization.to_state_dict(ip)\n",
    "    saved_param_dict.append(ip_dict)\n",
    "\n",
    "outtuple = (saved_param_dict, losses3)\n",
    "pickle.dump(outtuple, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne=200\n",
    "\n",
    "# x_epoch = jnp.array([range(ne)]).squeeze()\n",
    "\n",
    "# m1_fkl = jnp.array(losses1)[:ne,1]\n",
    "# m1_rkl = jnp.array(losses1)[:ne,2]\n",
    "\n",
    "# m2_fkl = jnp.array(losses2)[:ne,1]\n",
    "# m2_rkl = jnp.array(losses2)[:ne,2]\n",
    "\n",
    "# m3_fkl = jnp.array(losses3)[:ne,1]\n",
    "# m3_rkl = jnp.array(losses3)[:ne,2]\n",
    "\n",
    "# plt.plot(x_epoch, m1_fkl, '-', c='#1f77b4', label=r'$\\omega_k=0.1$: Likelihood')\n",
    "# plt.plot(x_epoch, m1_rkl, '--', c='#1f77b4', alpha=0.5, label=r'$\\omega_k=0.1$: Rev DKL')\n",
    "# plt.plot(x_epoch, m2_fkl, '-', c='#ff7f0e', label=r'$\\omega_k=0$: Likelihood')\n",
    "# plt.plot(x_epoch, m2_rkl, '--', c='#ff7f0e', alpha=0.5, label=r'$\\omega_k=0$: Rev DKL')\n",
    "# plt.plot(x_epoch, m3_fkl, '-', c='#15b01a', label=r'$\\omega_k=0$: Likelihood')\n",
    "# plt.plot(x_epoch, m3_rkl, '--', c='#15b01a', alpha=0.5, label=r'$\\omega_k=0$: Rev DKL')\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Force and RKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model4():\n",
    "    return SmoothNormalizingFlow(num_flows = 4,\n",
    "                                num_biject = 4,\n",
    "                                num_in_feat = nDimPS,\n",
    "                                cond_mlp_width = [[100,100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step4(state, batch, key):\n",
    "    def loss_fn(params):\n",
    "        \n",
    "        def _logp(x):\n",
    "            z, ldj = model4().apply({'params':params}, x, method=model4().inverse_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp + ldj)\n",
    "        \n",
    "        def synth_samp_logp_minus_logME(z):\n",
    "            x, ldj = model4().apply({'params':params}, z, method=model4().forward_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp - ldj) - vector_log_me(x).reshape(-1,1)\n",
    "        \n",
    "        def _force_MSE(x, grad_flow_logprob):\n",
    "            grad_logME = grad_vector_log_me_jit(x)\n",
    "            return jnp.square(grad_logME - grad_flow_logprob).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1)\n",
    "            \n",
    "        # FKL Loss\n",
    "        forward_kld_loss = (-1.0*_logp(batch)).mean()\n",
    "        \n",
    "        # RKL Loss\n",
    "        synth_z = jax.random.uniform(key, batch.shape)\n",
    "        reverse_kld_loss = (synth_samp_logp_minus_logME(synth_z)).mean()\n",
    "        \n",
    "        # Force Loss\n",
    "        _, _gradx_logp = model4().apply({'params':params}, batch, method=model4().val_and_gradx_logprob)           \n",
    "        force_mse = _force_MSE(batch, _gradx_logp).mean()\n",
    "        \n",
    "       \n",
    "        loss = forward_kld_loss + 0.1*reverse_kld_loss + (1.0e-6)*force_mse\n",
    "        return loss\n",
    "    \n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    \n",
    "    return state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step4_only_fkl(state, batch, key):\n",
    "    def loss_fn(params):\n",
    "        def _logp(x):\n",
    "            z, ldj = model4().apply({'params':params}, x, method=model4().inverse_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp + ldj)\n",
    "                \n",
    "        forward_kld_loss = (-1.0*_logp(batch)).mean()\n",
    "       \n",
    "        loss = forward_kld_loss \n",
    "        return loss\n",
    "    \n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    \n",
    "    return state.apply_gradients(grads=grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval4(params, batch, key):\n",
    "    def eval_model(smoothnf):\n",
    "        \n",
    "        def _logp(x):\n",
    "            z, ldj = model4().apply({'params':params}, x, method=model4().inverse_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp + ldj)\n",
    "        \n",
    "        def synth_samp_logp_minus_logME(z):\n",
    "            x, ldj = model4().apply({'params':params}, z, method=model4().forward_bijection)\n",
    "            base_logp = jax.scipy.stats.uniform.logpdf(z, loc=0, scale=1).sum(axis=tuple(range(1,z.ndim))).reshape(-1,1)\n",
    "            return (base_logp - ldj) - vector_log_me(x).reshape(-1,1)\n",
    "        \n",
    "        def _force_MSE_and_cos(x, grad_flow_logprob):\n",
    "            grad_logME = grad_vector_log_me_jit(x)\n",
    "            _force = jnp.square(grad_logME - grad_flow_logprob).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1)\n",
    "            \n",
    "            gme_norm = jnp.sqrt(jnp.square(grad_logME).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1))\n",
    "            gfl_norm = jnp.sqrt(jnp.square(grad_flow_logprob).sum(axis=tuple(range(1,x.ndim))).reshape(-1,1))\n",
    "            _cos = jnp.multiply(grad_logME, grad_flow_logprob) / (gme_norm + 1e-10) / (gfl_norm + 1e-10)\n",
    "            \n",
    "            return _force, _cos\n",
    "                \n",
    "        forward_kld_loss = (-1.0*_logp(batch)).mean()\n",
    "        #forward_kld_loss = (-1.0*_logp).mean()\n",
    "        \n",
    "        synth_z = jax.random.uniform(key, batch.shape)\n",
    "        reverse_kld_loss = synth_samp_logp_minus_logME(synth_z).mean()\n",
    "        \n",
    "        _, _gradx_logp = model4().apply({'params':params}, batch, method=model4().val_and_gradx_logprob)\n",
    "        _f, _c = _force_MSE_and_cos(batch, _gradx_logp)\n",
    "        force_mse = _f.mean()\n",
    "        cos_loss = _c.mean()\n",
    "        \n",
    "        metrics = {'fkld': forward_kld_loss,\n",
    "                   'rkld': reverse_kld_loss,\n",
    "                   'force': force_mse,\n",
    "                   'cos': cos_loss,\n",
    "                   'loss': forward_kld_loss + 0.1*reverse_kld_loss #+ 0.001*force_loss\n",
    "                  }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    return nn.apply(eval_model, model4())({'params': params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhe_events = pylhe.readLHE(\"./data/ee_ttbar_bqq_bqq/unweighted_events.lhe\")\n",
    "# events = get_multiple_lhe_event_to_ps_point(lhe_events, 10000)\n",
    "# dataset4 = vector_rv_from_ps_point(events, E_cm)\n",
    "# train_data4 = dataset4[0:8000]\n",
    "# test_data4 = dataset4[8000:]\n",
    "\n",
    "train_data4 = train_data\n",
    "test_data4 = test_data\n",
    "\n",
    "rng, key, eval_rng = jax.random.split(key, 3)\n",
    "\n",
    "# batch_size = 50\n",
    "# learning_rate = 1e-3\n",
    "init_data = jnp.ones((batch_size,nDimPS))\n",
    "\n",
    "#optimizer2 = optax.chain( optax.zero_nans(), optax.adam(learning_rate), optax.zero_nans())\n",
    "\n",
    "state4_init = train_state.TrainState.create(\n",
    "      apply_fn=model4().apply,\n",
    "      params=model4().init(key, init_data)['params'],\n",
    "      tx=optax.adam(learning_rate),\n",
    "  )\n",
    "\n",
    "state4 = state4_init\n",
    "\n",
    "\n",
    "#prejit:\n",
    "print(\"pre-eval model4\")\n",
    "batch = train_data4[np.random.choice(np.arange(len(train_data4)), size = batch_size)]\n",
    "start_time = timeit.default_timer()\n",
    "train_step4(state4, batch, key)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"train_step4: elapsed time\", elapsed,\"\\n\")\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "train_step4_only_fkl(state4, batch, key)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"train_step4_only_fkl:  elapsed time\", elapsed,\"\\n\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pre-jit eval4\")\n",
    "start_time = timeit.default_timer()\n",
    "metrics = eval4(state4.params, test_data4, eval_rng)\n",
    "print('eval4, fwdKLD: {:.4f}, revKLD: {:.4f}, forceMSE: {:.4f}, cosLoss: {:.4f} \\n'.format(\n",
    "        metrics['fkld'], metrics['rkld'], metrics['force'], metrics['cos']))\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print(\"elapsed time\", elapsed,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_params4 = []\n",
    "saved_params4.append(state4.params)\n",
    "\n",
    "losses4 = []\n",
    "\n",
    "state4 = state4_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state4 = state4_init\n",
    "# state4 = train_step1_only_fkl(state4, batch, key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 100\n",
    "# steps_per_epoch = 75 #20000 // batch_size\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch\",epoch)\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        \n",
    "        batch = train_data4[np.random.choice(np.arange(len(train_data4)), size = batch_size)]\n",
    "        rng, key = jax.random.split(rng)\n",
    "        if epoch < num_warm_up_epochs:\n",
    "            state4 = train_step4_only_fkl(state4, batch, key)\n",
    "        else:\n",
    "            state4 = train_step4(state4, batch, key)\n",
    "        \n",
    "    saved_params4.append(state4.params)\n",
    "        \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"elapsed time\", elapsed)\n",
    "    \n",
    "    metrics = eval4(state4.params, test_data4, eval_rng)\n",
    "    print('eval4 epoch: {}, fwdKLD: {:.4f}, revKLD: {:.4f}, forceMSE: {:.4f}, cosLoss: {:.4f} \\n'.format(\n",
    "        epoch, metrics['fkld'], metrics['rkld'], metrics['force'], metrics['cos']))\n",
    "    \n",
    "    losses4.append([metrics['fkld'], metrics['rkld'], metrics['force'], metrics['cos']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_name = \"results_comp_smoothNF/model4_wk0.1_wf1Em6_results2.pkl\"\n",
    "outfile = open(outfile_name,'wb')\n",
    "saved_param_dict = []\n",
    "for ip in saved_params4:\n",
    "    ip_dict = serialization.to_state_dict(ip)\n",
    "    saved_param_dict.append(ip_dict)\n",
    "\n",
    "outtuple = (saved_param_dict, losses4)\n",
    "pickle.dump(outtuple, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params1, all_losses1 = pickle.load( open( \"results_comp_smoothNF/model1_wk0.1_wf0_results2.pkl\", \"rb\" ) )\n",
    "all_params2, all_losses2 = pickle.load( open( \"results_comp_smoothNF/model2_wk0_wf0_results2.pkl\", \"rb\" ) )\n",
    "all_params3, all_losses3 = pickle.load( open( \"results_comp_smoothNF/model3_wk0_wf1Em6_results2.pkl\", \"rb\" ) )\n",
    "all_params4, all_losses4 = pickle.load( open( \"results_comp_smoothNF/model4_wk0.1_wf1Em6_results2.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns=12\n",
    "ne=212\n",
    "alpha=0.1\n",
    "x_epoch = jnp.array([range(ne-ns)]).squeeze()\n",
    "\n",
    "m1_fkl = jnp.array(all_losses1)[ns:ne,0]\n",
    "m2_fkl = jnp.array(all_losses2)[ns:ne,0]\n",
    "m3_fkl = jnp.array(all_losses3)[ns:ne,0]\n",
    "m4_fkl = jnp.array(all_losses4)[ns:ne,0]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x_epoch, m2_fkl, '-', c='#ff7f0e', alpha=0.7, label=r'$\\omega_k=0$,    $\\omega_f=0$')\n",
    "plt.plot(x_epoch, m1_fkl, '-', c='#1f77b4', alpha=0.7, label=r'$\\omega_k=0.1$, $\\omega_f=0$')\n",
    "plt.plot(x_epoch, m3_fkl, '-', c='#15b01a', alpha=0.7, label=r'$\\omega_k=0$,    $\\omega_f=10^{-5}$')\n",
    "plt.plot(x_epoch, m4_fkl, '-', c='#c20078', alpha=0.7, label=r'$\\omega_k=0.1$, $\\omega_f=10^{-5}$')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Likelihood Loss\")\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig('flow_likelihood.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "m1_rkl = jnp.array(all_losses1)[ns:ne,1]\n",
    "m2_rkl = jnp.array(all_losses2)[ns:ne,1]\n",
    "m3_rkl = jnp.array(all_losses3)[ns:ne,1]\n",
    "m4_rkl = jnp.array(all_losses4)[ns:ne,1]\n",
    "\n",
    "plt.plot(x_epoch, m2_rkl, '-', c='#ff7f0e', alpha=0.7, label=r'$\\omega_k=0$,    $\\omega_f=0$')\n",
    "plt.plot(x_epoch, m1_rkl, '-', c='#1f77b4', alpha=0.7, label=r'$\\omega_k=0.1$, $\\omega_f=0$')\n",
    "plt.plot(x_epoch, m3_rkl, '-', c='#15b01a', alpha=0.7, label=r'$\\omega_k=0$,    $\\omega_f=10^{-5}$')\n",
    "plt.plot(x_epoch, m4_rkl, '-', c='#c20078', alpha=0.7, label=r'$\\omega_k=0.1$, $\\omega_f=10^{-5}$')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Reverse KL Loss\")\n",
    "plt.legend()\n",
    "#plt.show()\n",
    "plt.savefig('flow_reverseKL.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, key = jax.random.split(rng) \n",
    "z1 = jax.random.uniform(key, (10000,14))\n",
    "x1, _ = model1().apply({'params':all_params1[ne]}, z1, method=model1().forward_bijection)\n",
    "\n",
    "rng, key = jax.random.split(rng)\n",
    "z2 = jax.random.uniform(key, (10000,14))\n",
    "x2, _ = model2().apply({'params':all_params2[ne]}, z2, method=model2().forward_bijection)\n",
    "\n",
    "rng, key = jax.random.split(rng) \n",
    "z3 = jax.random.uniform(key, (10000,14))\n",
    "x3, _ = model4().apply({'params':all_params3[ne]}, z3, method=model3().forward_bijection)\n",
    "\n",
    "rng, key = jax.random.split(rng)\n",
    "z4 = jax.random.uniform(key, (10000,14))\n",
    "x4, _ = model4().apply({'params':all_params4[ne]}, z4, method=model4().forward_bijection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = 1\n",
    "\n",
    "hbins = np.linspace(0,1,11)\n",
    "\n",
    "\n",
    "for idx in range(14):\n",
    "    plt.hist(train_data4[:,idx], bins=hbins, density=True, histtype='step', ec='black', lw=2)\n",
    "    plt.hist(x1[:,idx], bins=hbins, density=True, histtype='step', ec='#1f77b4')\n",
    "    plt.hist(x2[:,idx], bins=hbins, density=True, histtype='step', ec='#ff7f0e')\n",
    "    plt.hist(x3[:,idx], bins=hbins, density=True, histtype='step', ec='#15b01a')\n",
    "    plt.hist(x4[:,idx], bins=hbins, density=True, histtype='step', ec='#c20078')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne=19\n",
    "# metric = [\"total\", 'NLL', 'RKL $\\\\times 0.1$', 'FME $\\\\times 10^{-6}$']\n",
    "# weight = [1.0, 1.0, 0.1, 1.0e-6]\n",
    "\n",
    "# print('Metric', '&', '$\\\\omega_{RKL}=0$', '&', '$\\\\omega_{RKL}=0.1$', '&', '$\\\\omega_{RKL}=0$',       '&', '$\\\\omega_{RKL}=0.1$', \"\\\\\\\\\")\n",
    "# print(''      , '&', '$\\\\omega_{FME}=0$', '&', '$\\\\omega_{FME}=0$'  , '&', '$\\\\omega_{FME}=10^{-6}$', '&', '$\\\\omega_{FME}=10^{-6}$$', '\\\\\\\\ \\\\hline')\n",
    "# for iloss in range(1,4):\n",
    "#     print(metric[iloss], '&', \n",
    "#           \"{:.2f}\".format(weight[iloss]*jnp.asarray(losses2[ne][iloss])), '&', \n",
    "#           \"{:.2f}\".format(weight[iloss]*jnp.asarray(losses1[ne][iloss])), '&', \n",
    "#           \"{:.2f}\".format(weight[iloss]*jnp.asarray(losses3[ne][iloss])), '&', \n",
    "#           \"{:.2f}\".format(weight[iloss]*jnp.asarray(losses4[ne][iloss])), '\\\\\\\\ \\\\hline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _logp, _gradx_logp = model1().apply({'params':state3.params}, batch, method=model3().val_and_gradx_logprob)\n",
    "# print(_gradx_logp)\n",
    "# print(\" \")\n",
    "# me_gradlogp = grad_vector_log_me_jit(batch)\n",
    "# print(jnp.square(me_gradlogp-_gradx_logp).sum(axis=-1))\n",
    "\n",
    "\n",
    "# print(jnp.square(me_gradlogp - _gradx_logp).sum(axis=tuple(range(1,batch.ndim))).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff = lambda x: model3().apply({'params':state3.params}, x, method=model3().forward_bijection)\n",
    "#jax.jax.make_jaxpr(ff)(batch[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jax.jax.make_jaxpr(scalar_log_me)(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
